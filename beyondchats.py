# -*- coding: utf-8 -*-
"""BeyondChats.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lmgyhv-RDiYdUrCkO4jr155YYMCQniQl

**Installion of  Necessary Libraries**
"""

!pip install requests pandas

import requests
import pandas as pd
import time

"""**Fetching 1st 10 pages of Data from the API**"""

import requests
import json
import time

def fetch_data(api_url, page_break=None):
    page = 1
    all_data = {}
    retry_delay = 1  # Start with a 1-second delay

    while True:
        response = requests.get(f"{api_url}?page={page}")
        print(f"Fetching data from page {page}: Status {response.status_code}")  # Debug statement

        if response.status_code == 429:  # Handling rate limits
            print(f"Rate limit hit, retrying after {retry_delay} seconds...")
            time.sleep(retry_delay)
            retry_delay *= 2  # Exponentially increase the delay
            continue  # Retry the same page

        if response.status_code != 200:
            print("Failed to fetch data:", response.text)  # Show error response
            break

        try:
            data = response.json()
            print(data)
            all_data.update(data)  # Correctly update all_data with new data
        except json.JSONDecodeError:
            print("Failed to decode JSON:", response.text)  # Debug problematic text
            break

        if not data:  # Break if no more data is available
            break

        page += 1
        if page_break and page > page_break:
            break
        retry_delay = 1  # Reset the delay after a successful fetch

    return all_data
def find_citations(data):
    citations = []

    for item in data:
        if isinstance(item, str):  # Check if item is a string and attempt to convert it to dictionary
            try:
                item = json.loads(item)
            except json.JSONDecodeError:
                print("Failed to decode item from JSON:", item)
                continue  # Skip this item and continue with the next one

        if not isinstance(item, dict):
            print("Item is not a dictionary:", item)
            continue

        response_text = item.get('response', '')
        sources = item.get('sources', [])
        matched_sources = []

        for source in sources:
            if source['context'] in response_text:
                matched_source = {'id': source['id']}
                if source['context'] in response_text and source.get('link'):
                   matched_source = {'id': source['id'], 'link': source['link']}
                   matched_sources.append(matched_source)

        if matched_sources:
            citations.append({'response': response_text, 'citations': matched_sources})

    return citations

API_ENDPOINT = "https://devapi.beyondchats.com/api/get_message_with_sources"
all_data = fetch_data(API_ENDPOINT,page_break=10)

"""**saveing the fetched data**"""

import requests
import json

def fetch_and_store_data(api_url, output_file, start_page=1, end_page=10):
    with open(output_file, 'w') as file:
        file.write('[')  # Start of JSON array
        first_entry = True

        for page in range(start_page, end_page + 1):
            response = requests.get(f"{api_url}?page={page}")
            if response.status_code == 200:
                data = response.json()
                if not data:  # No more data available
                    break
                # Prepare JSON string for writing
                json_string = json.dumps(data, indent=2)
                if not first_entry:
                    file.write(',\n')  # Add comma before next entry, except before the first
                file.write(json_string)
                first_entry = False
            else:
                print(f"Failed to fetch data for page {page}: Status {response.status_code}")
                break

        file.write(']')  # End of JSON array

def main():
    API_ENDPOINT = "https://devapi.beyondchats.com/api/get_message_with_sources"
    OUTPUT_FILE = "fetched_data.json"
    START_PAGE = 1
    END_PAGE = 10
    fetch_and_store_data(API_ENDPOINT, OUTPUT_FILE, START_PAGE, END_PAGE)
    print(f"Data successfully written to {OUTPUT_FILE}")

"""**fetching the data using NLP**"""

def fetch_data(api_url, page_break=None):
    page = 1
    all_data = {}
    retry_delay = 1

    while True:
        response = requests.get(f"{api_url}?page={page}")
        if response.status_code == 429:
            time.sleep(retry_delay)
            retry_delay *= 2
            continue
        if response.status_code != 200:
            print("Failed to fetch data:", response.text)
            break

        try:
            data = response.json()
            if not isinstance(data, dict):
                print("Data is not a dictionary:", type(data))
                break
            all_data.update(data)
        except json.JSONDecodeError as e:
            print("Failed to decode JSON:", e)
            break

        if not data:  # Break if no more data is available
            break

        page += 1
        if page_break and page > page_break:
            break
        retry_delay = 1  # Reset the delay after a successful fetch

    return all_data

API_ENDPOINT = "https://devapi.beyondchats.com/api/get_message_with_sources"
all_data = fetch_data(API_ENDPOINT, page_break=10)
print(all_data)

"""**Fetching complete of Data & Returning the citations**"""

import requests
import json
import time

def fetch_data(api_url):
    page = 1
    all_data = {}
    retry_delay = 1  # Start with a 1-second delay

    while True:
        response = requests.get(f"{api_url}?page={page}")
        print(f"Fetching data from page {page}: Status {response.status_code}")  # Debug statement

        if response.status_code == 429:  # Handling rate limits
            print(f"Rate limit hit, retrying after {retry_delay} seconds...")
            time.sleep(retry_delay)
            retry_delay *= 2  # Exponentially increase the delay
            continue  # Retry the same page

        if response.status_code != 200:
            print("Failed to fetch data:", response.text)  # Show error response
            break

        try:
            data = response.json()
            print(data)
            all_data.update(data)  # Correctly update all_data with new data
        except json.JSONDecodeError:
            print("Failed to decode JSON:", response.text)  # Debug problematic text
            break

        if not data:  # Break if no more data is available
            break

        page += 1
        # if page_break and page > page_break:
        #     break
        retry_delay = 1  # Reset the delay after a successful fetch

    return all_data
def find_citations(data):
    citations = []

    for item in data:
        if isinstance(item, str):  # Check if item is a string and attempt to convert it to dictionary
            try:
                item = json.loads(item)

            except json.JSONDecodeError:
                print("Failed to decode item from JSON:", item)
                continue  # Skip this item and continue with the next one

        if not isinstance(item, dict):  # Ensure item is a dictionary
            print("Item is not a dictionary:", item)
            continue

        response_text = item.get('response', '')
        sources = item.get('sources', [])
        matched_sources = []

        for source in sources:
            if source['context'] in response_text:
                matched_source = {'id': source['id']}
                if source['context'] in response_text and source.get('link'):
                   matched_source = {'id': source['id'], 'link': source['link']}
                   matched_sources.append(matched_source)

        if matched_sources:
            citations.append({'response': response_text, 'citations': matched_sources})

    return citations

# def main():
API_ENDPOINT = "https://devapi.beyondchats.com/api/get_message_with_sources"
all_data = fetch_data(API_ENDPOINT)

"""**storeing the complete output**"""

def fetch_and_store_data(api_url, output_file, max_pages=10000):
    with open(output_file, 'w') as file:
        file.write('[')  # Start JSON array
        first_entry = True
        page = 1
        retry_delay = 1  # Initial delay of 1 second

        while page <= max_pages:
            try:
                response = requests.get(f"{api_url}?page={page}")
                if response.status_code == 200:
                    data = response.json()
                    if not data:  # No more data available
                        break
                    json_string = json.dumps(data, indent=2)
                    if not first_entry:
                        file.write(',\n')  # Add comma before next entry, except before the first
                    file.write(json_string)
                    first_entry = False
                    page += 1
                    retry_delay = 1  # Reset retry delay after a successful request
                elif response.status_code == 429:
                    print(f"Rate limit hit on page {page}, retrying after {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 60)  # Exponential backoff with a max delay of 60 seconds
                else:
                    print(f"Failed to fetch data for page {page}: Status {response.status_code}")
                    break
            except Exception as e:
                print(f"An error occurred: {str(e)}")
                break

        file.write(']')  # End JSON array
        print(f"Data fetching completed up to page {page-1}")

def main():
    API_ENDPOINT = "https://devapi.beyondchats.com/api/get_message_with_sources"
    OUTPUT_FILE = "complete_fetched_data.json"
    fetch_and_store_data(API_ENDPOINT, OUTPUT_FILE)
    print(f"Data successfully written to {OUTPUT_FILE}")

"""# **analyze citations and storing**"""

import json

def analyze_data(data):
    citations = []
    for item in data:
        # Checking if 'item' is a dictionary and contains 'response' key
        if isinstance(item, dict) and 'response' in item:
            response_text = item['response'].lower()  # Convert to lowercase for case-insensitive comparison
            sources = item.get('sources', [])  # Corrected from 'source' to 'sources' if that was a typo
            matched_sources = []

            for source in sources:
                contexts = source.get('context', [])
                if isinstance(contexts, list):
                    for context in contexts:
                        context = context.lower()  # Convert to lowercase
                        if context in response_text:
                            citation = {
                                'id': source['id'],
                                'context': context
                            }
                            if source.get('link'):
                                citation['link'] = source['link']
                            matched_sources.append(citation)
                            print(f"Match found: {context} in response.")

                elif isinstance(contexts, str):
                    contexts = contexts.lower()  # Convert to lowercase
                    if contexts in response_text:
                        citation = {
                            'id': source['id'],
                            'context': contexts
                        }
                        if source.get('link'):
                            citation['link'] = source['link']
                        matched_sources.append(citation)
                        print(f"Match found: {contexts} in response.")

            if matched_sources:
                citations.append({
                    'response': response_text,
                    'citations': matched_sources
                })
            else:
                citations.append({
                    'response': response_text,
                    'citations': []
                })

    return citations

def main():
    with open('fetched_data.json', 'r') as file:
        raw_data = json.load(file)

    citation_results = analyze_data(raw_data)  # Make sure the structure matches expected JSON path

    # Output the results to a file
    with open('citation_results.json', 'w') as outfile:
        json.dump(citation_results, outfile, indent=2)

    print("Analysis complete. Results are saved in 'citation_results.json'.")

if __name__ == "__main__":
    main()

def identify_sources(data):
    """Identify matching sources for each response and return citations."""
    citations = []
    for item in data:
        response = item['response']
        sources = item['sources']
        matched_sources = []
        for source in sources:
            if source['context'].lower() in response.lower():  # Using lower() to make the comparison case-insensitive
                matched_source = {"id": source['id']}
                if 'link' in source and source['link']:
                    matched_source['link'] = source['link']
                matched_sources.append(matched_source)
        citations.append(matched_sources if matched_sources else [])
    return citations

import json

# Load the JSON data
with open('fetched_data.json', 'r') as file:
    data = json.load(file)
responses_sources = [(item['response'], item['source']) for page in data for item in page['data']['data']]

from difflib import SequenceMatcher

def find_sources(response, sources):
    matched_sources = []
    for source in sources:
        # Simple substring matching; could be replaced with more sophisticated methods
        if SequenceMatcher(None, response, source['context']).ratio() > 0.1:
            matched_sources.append(source['id'])
    return matched_sources

# Analyze each response and find matching sources
results = {response: find_sources(response, sources) for response, sources in responses_sources}

print(json.dumps(results, indent=4))

"""**fetching the data using NLP**"""

# Install and import necessary packages
!pip install spacy
!python -m spacy download en_core_web_md

import spacy
import json

nlp = spacy.load("en_core_web_md")

# Define the function using NLP for semantic similarity
def find_citations(data):
    citations = []
    for item in data:
        response_text = nlp(item.get('response', ''))  # Convert response text to a spaCy Doc
        sources = item.get('sources', [])
        matched_sources = []

        for source in sources:
            source_doc = nlp(source['context'])  # Convert source context to a spaCy Doc
            if response_text.similarity(source_doc) > 0.75:  # Check if similarity is above a certain threshold
                matched_source = {'id': source['id']}
                if source.get('link'):
                    matched_source['link'] = source['link']
                matched_sources.append(matched_source)

        if matched_sources:
            citations.append({
                'response': response_text.text,
                'citations': matched_sources
            })

    return citations

# Sample data to test the function
sample_data = [
    {
        'response': "Climate change is causing increased sea levels and more frequent heatwaves.",
        'sources': [
            {'id': 1, 'context': "Global warming results in rising ocean levels.", 'link': 'http://example.com'},
            {'id': 2, 'context': "Economic growth has surged this year.", 'link': 'http://example2.com'}
        ]
    }
]
citations = find_citations(sample_data)
print(json.dumps(citations, indent=2))

"""**loading Tokenizer and model**"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_NAME = 'textattack/bert-base-uncased-SST-2'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

print("Tokenizer and model loaded successfully!")

"""example of the model

"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

MODEL_NAME = 'textattack/bert-base-uncased-SST-2'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

def find_sources_transformer(response, sources):
    matched_sources = []
    response_tokens = tokenizer.encode(response, add_special_tokens=True)

    for source in sources:
        source_tokens = tokenizer.encode(source['context'], add_special_tokens=True)

        # Managing token lengths to fit within model's maximum input length
        max_length = 512
        total_length = len(response_tokens) + len(source_tokens) + 1
        if total_length > max_length:
            half_max_length = (max_length - 3) // 2
            response_tokens = response_tokens[:half_max_length]
            source_tokens = source_tokens[:half_max_length]

        input_ids = [tokenizer.cls_token_id] + response_tokens + [tokenizer.sep_token_id] + source_tokens + [tokenizer.sep_token_id]
        input_ids = torch.tensor([input_ids])

        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits
            similarity = torch.sigmoid(logits).squeeze()

        if similarity[1] > 0.75:
            matched_sources.append({'id': source['id'], 'similarity': similarity[1].item()})

    return matched_sources

# Sample data for demonstration
responses_sources = [
    ("Climate change is affecting global temperatures.", [{'id': 1, 'context': "Global warming leads to higher temperatures."}])
]

# Using the transformer model to find matched sources for each response
results_transformer = {response: find_sources_transformer(response, sources) for response, sources in responses_sources}
print(results_transformer)

import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the data
with open('fetched_data.json', 'r') as file:
    data = json.load(file)

MODEL_NAME = 'textattack/bert-base-uncased-SST-2'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

def find_sources_transformer(response, sources):
    matched_sources = []
    # Check if response is a string
    if not isinstance(response, str):
        print(f"Skipping non-string response: {response}")
        return matched_sources

    for source in sources:
        context = source['context']
        # Ensure context is a string
        if not isinstance(context, str):
            print(f"Skipping source with non-string context (ID: {source['id']})")
            continue

        # Encode both the response and the source context with truncation and padding
        inputs = tokenizer.encode_plus(
            response, context, add_special_tokens=True,
            max_length=512, truncation=True, padding='max_length',
            return_tensors="pt"
        )

        input_ids = inputs['input_ids'].to(model.device)
        attention_mask = inputs['attention_mask'].to(model.device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            similarity = torch.sigmoid(logits).squeeze()

        # Assume class 1 is the 'similar' class
        if similarity[1] > 0.75:  # Adjust threshold as necessary
            matched_sources.append({'id': source['id'], 'similarity': similarity[1].item()})

    return matched_sources

results_transformer = {}
for item in data:
    responses_sources = item['data']['data']
    for entry in responses_sources:
        response = entry['response']
        sources = entry['source']
        results = find_sources_transformer(response, sources)
        results_transformer[response] = results

# Output results
print(json.dumps(results_transformer, indent=2))